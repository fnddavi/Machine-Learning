{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar MNIST\n",
    "print(\"Iniciando programa\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "print(\"MNIST carregado!\")\n",
    "\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(np.uint8)\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# 2. Classificação binária (é 5?)\n",
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "# 3. Treina o modelo\n",
    "clf = SGDClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train_5)\n",
    "\n",
    "# 4. Testar de 5 a 10 imagens aleatórias\n",
    "num_imgs = np.random.randint(5, 11)  # número aleatório entre 5 e 10\n",
    "random_indices = np.random.randint(0, len(X_test), size=num_imgs)\n",
    "\n",
    "# 5. Coletar amostras e fazer predições\n",
    "samples = X_test[random_indices]\n",
    "predictions = clf.predict(samples)\n",
    "\n",
    "# 6. Mostrar imagens com os resultados\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, (img, pred, idx) in enumerate(zip(samples, predictions, random_indices)):\n",
    "    plt.subplot(1, num_imgs, i + 1)\n",
    "    plt.imshow(img.reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(f\"É 5? {pred}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Classificação de dígitos aleatórios\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MATRIZ DE CONFUSÃO\n",
    "# Previsões\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Matriz de confusão (classe real x predita)\n",
    "matriz = confusion_matrix(y_test_5, y_pred)\n",
    "\n",
    "# Exibir numericamente\n",
    "print(\"Matriz de confusão:\")\n",
    "print(matriz)\n",
    "\n",
    "# Exibir visualmente\n",
    "disp = ConfusionMatrixDisplay(matriz, display_labels=[\"Não é 5\", \"É 5\"])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Matriz de Confusão\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fórmulas\n",
    "### Precisão\n",
    "Das vezes que o modelo disse \"é 5\", quantas ele acertou?\n",
    "- Quando utilizar?\n",
    "    Você não quer falsos positivos(ex: sistema judicial, alertas falsos)\n",
    "\n",
    "PRECISAO = TP / TP + FP\n",
    "\n",
    "### Revocação ( Recall ou Sensibilidade)\n",
    "Das vezes que realmente era \"5\", quantas ele acertou?\n",
    "- Quando utilizar?\n",
    "    Você não pode perder positivos reais(ex: câncer, fraude, incêndio)\n",
    "\n",
    "REVOCACAO = TP / TP + FN\n",
    "\n",
    "### F1-score (Equilíbrio entre precisão e revocação)\n",
    "- Quando utilizar?\n",
    "    Quer um equilibrio entre precisão e revocação, especialmente em classes desbalanceadas.\n",
    "    \n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Previsões no conjunto de teste\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Métricas\n",
    "precisao = precision_score(y_test_5, y_pred)\n",
    "revocacao = recall_score(y_test_5, y_pred)\n",
    "f1 = f1_score(y_test_5, y_pred)\n",
    "\n",
    "print(f\"Precisão: {precisao:.4f}\")\n",
    "print(f\"Revocação: {revocacao:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade-off entre precisão e revocação\n",
    "A maioria dos classificadores, como o SGDClassifier, faz previsões baseadas em pontuações (scores), e não apenas \"sim ou não\". Por exemplo:\n",
    "\n",
    "´´´clf.decision_function([amostra])´´´\n",
    "\n",
    "Essa função retorna um valor contínuo. O Scikit-Learn, por padrão, usa threshold = 0 para decidir:\n",
    "\n",
    "- Se score ≥ 0 → classe positiva (“é 5”)\n",
    "\n",
    "- Se score < 0 → classe negativa (“não é 5”)\n",
    "\n",
    "### Se mudarmos esse threshold...\n",
    "' podemos \"forçar\" o modelo a ser mais cauteloso ou mais ousado.\n",
    "\n",
    "- Se exigirmos score >= +1, ele só diz que é 5 se tiver muita certeza (+ precisão, - revocação)\n",
    "\n",
    "- Se aceitarmos score >= -1, ele marca \"é 5\" mesmo sem tanta certeza (- precisão, + revocação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "# Obtemos os scores de decisão para todo o conjunto de teste \n",
    "scores = clf.decision_function(X_test)\n",
    "\n",
    "# Calculamos os valores de precisão e revocação em vários thresholds\n",
    "precisions,recalls,thresholds = precision_recall_curve(y_test_5,scores)\n",
    "\n",
    "# Plotando\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precisão\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Revocação\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.title(\"Trade-off entre Precisão e Revocação\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALTERANDO O THRESHOLD DO NOSSO EXEMPLO DE CLASSIFICAÇÃO\n",
    "random_indices = np.random.randint(0, len(X_test), size=num_imgs)\n",
    "# y reais para os samples aleatórios\n",
    "y_real = y_test_5[random_indices]\n",
    "\n",
    "# 5. Coletar amostras e fazer predições\n",
    "samples = X_test[random_indices]\n",
    "# 5. Coletar amostras e fazer predições\n",
    "scores = clf.decision_function(samples)\n",
    "\n",
    "# 5.5 Escolhe o threshold desejado\n",
    "thresholds = 1.025\n",
    "predictions = scores > thresholds\n",
    "\n",
    "# 6. Mostrar imagens com os resultados\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, (img, pred, idx) in enumerate(zip(samples, predictions, random_indices)):\n",
    "    plt.subplot(1, num_imgs, i + 1)\n",
    "    plt.imshow(img.reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(f\"É 5? {pred}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Classificação de dígitos aleatórios\")\n",
    "plt.show()\n",
    "\n",
    "# Calcular métricas com base nas novas previsões \n",
    "precisao = precision_score(y_real, predictions, zero_division=0)\n",
    "revocacao = recall_score(y_real, predictions, zero_division=0)\n",
    "f1 = f1_score(y_real, predictions, zero_division=0)\n",
    "\n",
    "\n",
    "# Exibir resultados\n",
    "print(f\"Threshold: {thresholds}\")\n",
    "print(f\"Precisão: {precisao:.4f}\")\n",
    "print(f\"Revocação: {revocacao:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
